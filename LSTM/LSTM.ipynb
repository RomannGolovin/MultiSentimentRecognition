{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd8aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef1e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import gensim.downloader\n",
    "# import string\n",
    "import gensim\n",
    "from string import punctuation\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d6c3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc4d72d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>dataset</th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "      <th>Google</th>\n",
       "      <th>multi_language</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-מעשי הטרוריזם האחרונים של קיין... שביניהם פיצ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>XED</td>\n",
       "      <td>sentence</td>\n",
       "      <td>he</td>\n",
       "      <td>-kane's recent acts of terrorism... including ...</td>\n",
       "      <td>0</td>\n",
       "      <td>kane s recent acts of terrorism including the ...</td>\n",
       "      <td>kane recent act terrorism including explosion ...</td>\n",
       "      <td>kane's recent acts of terrorism including the ...</td>\n",
       "      <td>kane's recent act terrorism including explosio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>- а помните как раньше с фруктов наклейки поо...</td>\n",
       "      <td>1</td>\n",
       "      <td>RuTweetCorp</td>\n",
       "      <td>tweet</td>\n",
       "      <td>ru</td>\n",
       "      <td>- and remember how we used to stick stickers o...</td>\n",
       "      <td>0</td>\n",
       "      <td>and remember how we used to stick stickers on ...</td>\n",
       "      <td>remember used stick sticker nail fruit busines...</td>\n",
       "      <td>and remember how we used to stick stickers on ...</td>\n",
       "      <td>remember used stick sticker nail fruit busines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>- если обычная девушка - загадка, то я супер ...</td>\n",
       "      <td>1</td>\n",
       "      <td>RuTweetCorp</td>\n",
       "      <td>tweet</td>\n",
       "      <td>ru</td>\n",
       "      <td>- if an ordinary girl is a mystery, then i am ...</td>\n",
       "      <td>0</td>\n",
       "      <td>if an ordinary girl is a mystery then i am a s...</td>\n",
       "      <td>ordinary girl mystery super secret state secret</td>\n",
       "      <td>if an ordinary girl is a mystery then i am a s...</td>\n",
       "      <td>ordinary girl mystery super secret state secre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- парни-это подобие унитазов: или заняты, или...</td>\n",
       "      <td>1</td>\n",
       "      <td>RuTweetCorp</td>\n",
       "      <td>tweet</td>\n",
       "      <td>ru</td>\n",
       "      <td>- guys are like toilets: either busy or shitty)</td>\n",
       "      <td>0</td>\n",
       "      <td>guys are like toilets either busy or shitty</td>\n",
       "      <td>guy like toilet either busy shitty</td>\n",
       "      <td>guys are like toilets either busy or shitty</td>\n",
       "      <td>guy like toilet either busy shitty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- я так то добрый , но на хуй вам придется вс...</td>\n",
       "      <td>1</td>\n",
       "      <td>RuTweetCorp</td>\n",
       "      <td>tweet</td>\n",
       "      <td>ru</td>\n",
       "      <td>- i'm so kind, but fuck you you still have to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>i m so kind but fuck you you still have to go</td>\n",
       "      <td>kind fuck still go</td>\n",
       "      <td>i'm so kind but fuck you you still have to go :-)</td>\n",
       "      <td>kind fuck still go :-)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label      dataset  \\\n",
       "0  -מעשי הטרוריזם האחרונים של קיין... שביניהם פיצ...     -1          XED   \n",
       "1   - а помните как раньше с фруктов наклейки поо...      1  RuTweetCorp   \n",
       "2   - если обычная девушка - загадка, то я супер ...      1  RuTweetCorp   \n",
       "3   - парни-это подобие унитазов: или заняты, или...      1  RuTweetCorp   \n",
       "4   - я так то добрый , но на хуй вам придется вс...      1  RuTweetCorp   \n",
       "\n",
       "       type language                                             Google  \\\n",
       "0  sentence       he  -kane's recent acts of terrorism... including ...   \n",
       "1     tweet       ru  - and remember how we used to stick stickers o...   \n",
       "2     tweet       ru  - if an ordinary girl is a mystery, then i am ...   \n",
       "3     tweet       ru    - guys are like toilets: either busy or shitty)   \n",
       "4     tweet       ru  - i'm so kind, but fuck you you still have to ...   \n",
       "\n",
       "   multi_language                                              token  \\\n",
       "0               0  kane s recent acts of terrorism including the ...   \n",
       "1               0  and remember how we used to stick stickers on ...   \n",
       "2               0  if an ordinary girl is a mystery then i am a s...   \n",
       "3               0        guys are like toilets either busy or shitty   \n",
       "4               0      i m so kind but fuck you you still have to go   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  kane recent act terrorism including explosion ...   \n",
       "1  remember used stick sticker nail fruit busines...   \n",
       "2    ordinary girl mystery super secret state secret   \n",
       "3                 guy like toilet either busy shitty   \n",
       "4                                 kind fuck still go   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  kane's recent acts of terrorism including the ...   \n",
       "1  and remember how we used to stick stickers on ...   \n",
       "2  if an ordinary girl is a mystery then i am a s...   \n",
       "3        guys are like toilets either busy or shitty   \n",
       "4  i'm so kind but fuck you you still have to go :-)   \n",
       "\n",
       "                                         tweet_lemma  \n",
       "0  kane's recent act terrorism including explosio...  \n",
       "1  remember used stick sticker nail fruit busines...  \n",
       "2  ordinary girl mystery super secret state secre...  \n",
       "3                 guy like toilet either busy shitty  \n",
       "4                             kind fuck still go :-)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('../df_clean_lemma.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce7c2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(df, text_column, label_column, datatype_column, datatype, fillna, shuffle_flg):\n",
    "    if fillna=='drop':\n",
    "        df.dropna().reset_index(drop=True,inplace=False)\n",
    "    else:\n",
    "        if type(fillna[0]) == str:\n",
    "            values = {text_column: fillna[0], label_column: fillna[1]}\n",
    "            if use_datatype:\n",
    "                values[datatype_column] = fillna[0]\n",
    "        else:\n",
    "            values = {text_column: fillna[1], label_column: fillna[0]}\n",
    "            if datatype_column is not None:\n",
    "                values[datatype_column] = fillna[1]            \n",
    "        df.fillna(value=values)\n",
    "    df = df[df[text_column].isna()==False]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    if datatype_column is not None and datatype != 'all':\n",
    "        df = df[df[datatype_column] == datatype]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    if shuffle_flg:\n",
    "        df = shuffle(df)\n",
    "    values = None, None; del values; gc.collect()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c585eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_name = 'glove.6B.300d.txt'\n",
    "try:\n",
    "    word2vec = gensim.downloader.load(emb_name)\n",
    "except:\n",
    "    try:\n",
    "        word2vec = KeyedVectors.load_word2vec_format('./Эмбеддинги/'+emb_name, binary=False)\n",
    "    except:\n",
    "        word2vec = KeyedVectors.load_word2vec_format('./Эмбеддинги/'+emb_name, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e62fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(word):\n",
    "    if word in word2idx.keys():\n",
    "        return word2idx[word]\n",
    "    return word2idx[\"unk\"]\n",
    "word2idx = {word: idx for idx, word in enumerate(word2vec.index_to_key )}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea29528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_emb(emb_vectors, sent, emb_type, min_length, max_length):\n",
    "    ans = []\n",
    "    flag = True\n",
    "    word_count = 0\n",
    "    if emb_type == 'mean':\n",
    "        if type(sent)==str:\n",
    "            sent = sent.split()\n",
    "            for word in sent:\n",
    "                try:\n",
    "                    if flag:\n",
    "                        ans = emb_vectors[word]\n",
    "                        word_count += 1\n",
    "                        flag = False\n",
    "                    else:\n",
    "                        ans = list(map(sum, zip(ans, emb_vectors[word])))\n",
    "                        word_count += 1\n",
    "                except:\n",
    "                    continue\n",
    "        elif type(sent)==list:\n",
    "            sent = sent\n",
    "            for word in sent:\n",
    "                try:\n",
    "                    if flag:\n",
    "                        ans = emb_vectors[word]\n",
    "                        word_count += 1\n",
    "                        flag = False\n",
    "                    else:\n",
    "                        ans = list(map(sum, zip(ans, emb_vectors[word])))\n",
    "                        word_count += 1\n",
    "                except:\n",
    "                    continue   \n",
    "        if word_count > min_length and word_count < max_length:\n",
    "            ans = list(np.array(ans) / word_count)\n",
    "            sent = None; del sent; \n",
    "            return ans\n",
    "        else:\n",
    "            ans = list(np.array(ans) / word_count)\n",
    "            ans, sent = None, None; del ans, sent; \n",
    "            return []        \n",
    "    elif emb_type == 'sequence':\n",
    "        if type(sent)==str:\n",
    "            sent = sent.split()\n",
    "            for word in sent:\n",
    "                try:\n",
    "                    if flag:\n",
    "                        ans = emb_vectors[word]\n",
    "                        word_count += 1\n",
    "                        flag = False\n",
    "                    else:\n",
    "                        ans = ans.append(emb_vectors[word])\n",
    "                        word_count += 1\n",
    "                except:\n",
    "                    continue\n",
    "        elif type(sent)==list:\n",
    "            sent = sent\n",
    "            for word in sent:\n",
    "                try:\n",
    "                    if flag:\n",
    "                        ans = [emb_vectors[word]]\n",
    "                        word_count += 1\n",
    "                        flag = False\n",
    "                    else:\n",
    "                        ans.append(emb_vectors[word])\n",
    "                        word_count += 1\n",
    "                except:\n",
    "                    continue   \n",
    "        if word_count > min_length and word_count < max_length:\n",
    "            sent = None; del sent; \n",
    "            return ans\n",
    "        else:\n",
    "            ans, sent = None, None; del ans, sent; \n",
    "            return []   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba140183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>dataset</th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "      <th>Google</th>\n",
       "      <th>multi_language</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>293969</th>\n",
       "      <td>sam harper was a marine who was killed by frie...</td>\n",
       "      <td>0</td>\n",
       "      <td>Large movie review dataset</td>\n",
       "      <td>review</td>\n",
       "      <td>en</td>\n",
       "      <td>sam harper was a marine who was killed by frie...</td>\n",
       "      <td>0</td>\n",
       "      <td>sam harper was a marine who was killed by frie...</td>\n",
       "      <td>sam harper marine killed friendly fire war kuw...</td>\n",
       "      <td>sam harper was a marine who was killed by frie...</td>\n",
       "      <td>sam harper marine killed friendly fire war kuw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461247</th>\n",
       "      <td>отличный отель останавливались с женой во врем...</td>\n",
       "      <td>2</td>\n",
       "      <td>clothing e-comm</td>\n",
       "      <td>review</td>\n",
       "      <td>ru</td>\n",
       "      <td>excellent hotel, my wife and i stayed during c...</td>\n",
       "      <td>0</td>\n",
       "      <td>excellent hotel my wife and i stayed during ch...</td>\n",
       "      <td>excellent hotel wife stayed check got free roo...</td>\n",
       "      <td>excellent hotel my wife and i stayed during ch...</td>\n",
       "      <td>excellent hotel wife stayed check-in got free ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159219</th>\n",
       "      <td>excellent product hair striaght dry matter min...</td>\n",
       "      <td>2</td>\n",
       "      <td>Multi-Domain</td>\n",
       "      <td>review</td>\n",
       "      <td>en</td>\n",
       "      <td>excellent product hair striaght dry matter min...</td>\n",
       "      <td>0</td>\n",
       "      <td>excellent product hair striaght dry matter min...</td>\n",
       "      <td>excellent product hair striaght dry matter minute</td>\n",
       "      <td>excellent product hair striaght dry matter min...</td>\n",
       "      <td>excellent product hair striaght dry matter minute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121086</th>\n",
       "      <td>almost no martial arts and what there was, app...</td>\n",
       "      <td>2</td>\n",
       "      <td>Large movie review dataset</td>\n",
       "      <td>review</td>\n",
       "      <td>en</td>\n",
       "      <td>almost no martial arts and what there was, app...</td>\n",
       "      <td>0</td>\n",
       "      <td>almost no martial arts and what there was appe...</td>\n",
       "      <td>almost martial art appeared simulated camera f...</td>\n",
       "      <td>almost no martial arts and what there was appe...</td>\n",
       "      <td>almost martial art appeared simulated camera f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287681</th>\n",
       "      <td>rt @t3hd1g17: @mihhamd это прозвучало как \"вы ...</td>\n",
       "      <td>2</td>\n",
       "      <td>RuTweetCorp</td>\n",
       "      <td>tweet</td>\n",
       "      <td>ru</td>\n",
       "      <td>rt @t3hd1g17: @mihhamd it sounded like \"do you...</td>\n",
       "      <td>0</td>\n",
       "      <td>rt t3hd1g17 mihhamd it sounded like do you wan...</td>\n",
       "      <td>rt t3hd1g17 mihhamd sounded like want believe ...</td>\n",
       "      <td>rt @t3hd1g17 @mihhamd it sounded like do you w...</td>\n",
       "      <td>rt @t3hd1g17 @mihhamd sounded like want believ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492892</th>\n",
       "      <td>сначала о плюсах приветливый персонал вкусная ...</td>\n",
       "      <td>1</td>\n",
       "      <td>clothing e-comm</td>\n",
       "      <td>review</td>\n",
       "      <td>ru</td>\n",
       "      <td>first, about the pros: friendly staff, delicio...</td>\n",
       "      <td>0</td>\n",
       "      <td>first about the pros friendly staff delicious ...</td>\n",
       "      <td>first pro friendly staff delicious cuisine pro...</td>\n",
       "      <td>first about the pros friendly staff delicious ...</td>\n",
       "      <td>first pro friendly staff delicious cuisine pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403600</th>\n",
       "      <td>и социопатите, които контролират тези програми...</td>\n",
       "      <td>1</td>\n",
       "      <td>XED</td>\n",
       "      <td>sentence</td>\n",
       "      <td>bg</td>\n",
       "      <td>and the sociopaths who control these programs ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and the sociopaths who control these programs ...</td>\n",
       "      <td>sociopath control program must ask stop talkin...</td>\n",
       "      <td>and the sociopaths who control these programs ...</td>\n",
       "      <td>sociopath control program must ask stop talkin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291898</th>\n",
       "      <td>rt @yarishka_cao: ебааать, на майдане стоят ми...</td>\n",
       "      <td>0</td>\n",
       "      <td>RuTweetCorp</td>\n",
       "      <td>tweet</td>\n",
       "      <td>ru</td>\n",
       "      <td>rt @yarishka_cao: fuck, there are millions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>rt yarishka cao fuck there are millions on the...</td>\n",
       "      <td>rt yarishka cao fuck million maidan russia say...</td>\n",
       "      <td>rt @yarishka_cao fuck there are millions on th...</td>\n",
       "      <td>rt @yarishka_cao fuck million maidan russia sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117782</th>\n",
       "      <td>adică cel pe care l-a lăsat înăuntru a deschis...</td>\n",
       "      <td>0</td>\n",
       "      <td>XED</td>\n",
       "      <td>sentence</td>\n",
       "      <td>ro</td>\n",
       "      <td>so whoever he let in opened the windows to mak...</td>\n",
       "      <td>0</td>\n",
       "      <td>so whoever he let in opened the windows to mak...</td>\n",
       "      <td>whoever let opened window make look like burglary</td>\n",
       "      <td>so whoever he let in opened the windows to mak...</td>\n",
       "      <td>whoever let opened window make look like burglary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273408</th>\n",
       "      <td>rt @juliabalandina: молодежка с каждой серией ...</td>\n",
       "      <td>2</td>\n",
       "      <td>RuTweetCorp</td>\n",
       "      <td>tweet</td>\n",
       "      <td>ru</td>\n",
       "      <td>rt @juliabalandina: youth is getting more and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>rt juliabalandina youth is getting more and mo...</td>\n",
       "      <td>rt juliabalandina youth getting interesting ep...</td>\n",
       "      <td>rt @juliabalandina youth is getting more and m...</td>\n",
       "      <td>rt @juliabalandina youth getting interesting e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>584766 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "293969  sam harper was a marine who was killed by frie...      0   \n",
       "461247  отличный отель останавливались с женой во врем...      2   \n",
       "159219  excellent product hair striaght dry matter min...      2   \n",
       "121086  almost no martial arts and what there was, app...      2   \n",
       "287681  rt @t3hd1g17: @mihhamd это прозвучало как \"вы ...      2   \n",
       "...                                                   ...    ...   \n",
       "492892  сначала о плюсах приветливый персонал вкусная ...      1   \n",
       "403600  и социопатите, които контролират тези програми...      1   \n",
       "291898  rt @yarishka_cao: ебааать, на майдане стоят ми...      0   \n",
       "117782  adică cel pe care l-a lăsat înăuntru a deschis...      0   \n",
       "273408  rt @juliabalandina: молодежка с каждой серией ...      2   \n",
       "\n",
       "                           dataset      type language  \\\n",
       "293969  Large movie review dataset    review       en   \n",
       "461247             clothing e-comm    review       ru   \n",
       "159219                Multi-Domain    review       en   \n",
       "121086  Large movie review dataset    review       en   \n",
       "287681                 RuTweetCorp     tweet       ru   \n",
       "...                            ...       ...      ...   \n",
       "492892             clothing e-comm    review       ru   \n",
       "403600                         XED  sentence       bg   \n",
       "291898                 RuTweetCorp     tweet       ru   \n",
       "117782                         XED  sentence       ro   \n",
       "273408                 RuTweetCorp     tweet       ru   \n",
       "\n",
       "                                                   Google  multi_language  \\\n",
       "293969  sam harper was a marine who was killed by frie...               0   \n",
       "461247  excellent hotel, my wife and i stayed during c...               0   \n",
       "159219  excellent product hair striaght dry matter min...               0   \n",
       "121086  almost no martial arts and what there was, app...               0   \n",
       "287681  rt @t3hd1g17: @mihhamd it sounded like \"do you...               0   \n",
       "...                                                   ...             ...   \n",
       "492892  first, about the pros: friendly staff, delicio...               0   \n",
       "403600  and the sociopaths who control these programs ...               0   \n",
       "291898  rt @yarishka_cao: fuck, there are millions on ...               0   \n",
       "117782  so whoever he let in opened the windows to mak...               0   \n",
       "273408  rt @juliabalandina: youth is getting more and ...               0   \n",
       "\n",
       "                                                    token  \\\n",
       "293969  sam harper was a marine who was killed by frie...   \n",
       "461247  excellent hotel my wife and i stayed during ch...   \n",
       "159219  excellent product hair striaght dry matter min...   \n",
       "121086  almost no martial arts and what there was appe...   \n",
       "287681  rt t3hd1g17 mihhamd it sounded like do you wan...   \n",
       "...                                                   ...   \n",
       "492892  first about the pros friendly staff delicious ...   \n",
       "403600  and the sociopaths who control these programs ...   \n",
       "291898  rt yarishka cao fuck there are millions on the...   \n",
       "117782  so whoever he let in opened the windows to mak...   \n",
       "273408  rt juliabalandina youth is getting more and mo...   \n",
       "\n",
       "                                                    lemma  \\\n",
       "293969  sam harper marine killed friendly fire war kuw...   \n",
       "461247  excellent hotel wife stayed check got free roo...   \n",
       "159219  excellent product hair striaght dry matter minute   \n",
       "121086  almost martial art appeared simulated camera f...   \n",
       "287681  rt t3hd1g17 mihhamd sounded like want believe ...   \n",
       "...                                                   ...   \n",
       "492892  first pro friendly staff delicious cuisine pro...   \n",
       "403600  sociopath control program must ask stop talkin...   \n",
       "291898  rt yarishka cao fuck million maidan russia say...   \n",
       "117782  whoever let opened window make look like burglary   \n",
       "273408  rt juliabalandina youth getting interesting ep...   \n",
       "\n",
       "                                              tweet_token  \\\n",
       "293969  sam harper was a marine who was killed by frie...   \n",
       "461247  excellent hotel my wife and i stayed during ch...   \n",
       "159219  excellent product hair striaght dry matter min...   \n",
       "121086  almost no martial arts and what there was appe...   \n",
       "287681  rt @t3hd1g17 @mihhamd it sounded like do you w...   \n",
       "...                                                   ...   \n",
       "492892  first about the pros friendly staff delicious ...   \n",
       "403600  and the sociopaths who control these programs ...   \n",
       "291898  rt @yarishka_cao fuck there are millions on th...   \n",
       "117782  so whoever he let in opened the windows to mak...   \n",
       "273408  rt @juliabalandina youth is getting more and m...   \n",
       "\n",
       "                                              tweet_lemma  \n",
       "293969  sam harper marine killed friendly fire war kuw...  \n",
       "461247  excellent hotel wife stayed check-in got free ...  \n",
       "159219  excellent product hair striaght dry matter minute  \n",
       "121086  almost martial art appeared simulated camera f...  \n",
       "287681  rt @t3hd1g17 @mihhamd sounded like want believ...  \n",
       "...                                                   ...  \n",
       "492892  first pro friendly staff delicious cuisine pro...  \n",
       "403600  sociopath control program must ask stop talkin...  \n",
       "291898  rt @yarishka_cao fuck million maidan russia sa...  \n",
       "117782  whoever let opened window make look like burglary  \n",
       "273408  rt @juliabalandina youth getting interesting e...  \n",
       "\n",
       "[584766 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = data_preprocess(df, text_column='lemma', label_column='label', datatype_column='dataset', datatype='all', fillna='drop', shuffle_flg=True)\n",
    "df1['label'] = [df1['label'][i]+1 for i in range(df1.shape[0])]\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "688d5a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator(object):\n",
    "    def __init__(self, true_max_len, x_column, y_column):\n",
    "        self.true_max_len = true_max_len\n",
    "    def __call__(self, batch):\n",
    "        max_len = max(len(row[x_column]) for row in batch)\n",
    "        input_embeds = []\n",
    "        labels = torch.empty(len(batch), dtype=torch.long)\n",
    "        for idx, row in enumerate(batch):\n",
    "            input_embeds.append(row[x_column])\n",
    "            labels[idx] = row[y_column]\n",
    "        input_embeds[0] = nn.ConstantPad1d((0, self.true_max_len - input_embeds[0].shape[0]), 0)(input_embeds[0])\n",
    "        input_embeds = pad_sequence(input_embeds,batch_first=True)\n",
    "        input_embeds = torch.Tensor(input_embeds)\n",
    "        return {x_column: input_embeds, y_column: labels}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee8f3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(df, text_column, label_column, model, \n",
    "                      emb_type, min_length, max_length, train_test_rate, \n",
    "                      datatype_column, test_type):\n",
    "    if datatype_column is not None:\n",
    "        df = df[[text_column, label_column, datatype_column]]\n",
    "    else:\n",
    "        df = df[[text_column, label_column]]\n",
    "    df['token'] = [[]]*df.shape[0]\n",
    "    if emb_type == 'mean':\n",
    "        for i in trange(df.shape[0], leave=False, desc='Applying emb'):\n",
    "            temp = list(df[text_column][i].split())[:max_length]\n",
    "            temp2 = make_emb(model, temp, emb_type, min_length, max_length)\n",
    "            df['token'][i] = temp2\n",
    "            temp = None; del temp\n",
    "    elif emb_type == 'sequence':\n",
    "        for i in trange(df.shape[0], leave=False, desc='Applying emb'):\n",
    "            temp = list(df[text_column][i].split())[:max_length]\n",
    "            temp2 = [encode(word) for word in temp]\n",
    "            if len(temp2) < min_length or len(temp2) > max_length:\n",
    "                temp2 = []\n",
    "            df['token'][i] = temp2\n",
    "            temp = None; del temp\n",
    "    df = df.loc[df['token'].isin([[]])==False]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    if train_test_rate > 0.:\n",
    "        if test_type == 'all':\n",
    "            train_df, test_df = train_test_split(df, test_size=train_test_rate, random_state=42,stratify=df[label_column])\n",
    "        else:\n",
    "            train_df = df[df['dataset'] != test_type]\n",
    "            test_df = df[df['dataset'] == test_type]         \n",
    "        train_df.reset_index(drop=True, inplace=True)\n",
    "        test_df.reset_index(drop=True, inplace=True)      \n",
    "        return train_df, test_df\n",
    "    else:\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e6e9c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying emb:   0%|          | 0/584766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 128\n",
    "\n",
    "# collate_fn = Collator(max_length,x_column=x_column, y_column=y_column)\n",
    "# train_df, test_df = create_embeddings(df=df1, text_column='lemma', label_column='label',\n",
    "#                             model=word2vec, emb_type='sequence', min_length=3, \n",
    "#                             max_length=max_length,  train_test_rate = 0.25,\n",
    "#                             datatype_column='dataset',test_type='all', balancing='no',\n",
    "#                             batch_size=512, collate_fn=collate_fn)\n",
    "df_all = create_embeddings(df=df1, text_column='lemma', label_column='label',\n",
    "                            model=word2vec, emb_type='sequence', min_length=3, \n",
    "                            max_length=max_length, train_test_rate = 0.,\n",
    "                            datatype_column='dataset', test_type='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d423a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight)             \n",
    "    elif isinstance(m, nn.LSTM) or isinstance(m, nn.GRU) or isinstance(m, nn.RNN):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fa4d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, use_embed_layer, embed_size, hidden_size, drop_rate, sequence_length, layer_type, bidir_flg, num_layers=6, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.num_layers = num_layers        \n",
    "        self.layer_type = layer_type\n",
    "        self.bidir = True if bidir_flg == 1 else False\n",
    "        self.drop_in_gru = drop_rate\n",
    "      \n",
    "        if use_embed_layer:\n",
    "            self.embed = nn.Embedding(len(word2idx), embed_size)\n",
    "        else:\n",
    "            self.embed = nn.Linear(len(word2idx), embed_size)\n",
    "        if self.layer_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=self.bidir,dropout=self.drop_in_gru)\n",
    "        elif self.layer_type == 'rnn':\n",
    "            self.rnn = nn.RNN(embed_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=self.bidir,dropout=self.drop_in_gru)\n",
    "        if self.layer_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=self.bidir,dropout=self.drop_in_gru)\n",
    "        self.fc = nn.Linear(hidden_size*sequence_length*2 if self.bidir else hidden_size*sequence_length, num_classes)\n",
    "        self.drop = nn.Dropout(drop_rate)\n",
    "\n",
    "    def init_hidden(self, batch_size, layer_type):\n",
    "        if layer_type == 'lstm':\n",
    "            return(autograd.Variable(torch.randn(self.num_layers*2 if self.bidir else self.num_layers, batch_size, self.hidden_size)).to(device),\n",
    "               autograd.Variable(torch.randn(self.num_layers*2 if self.bidir else self.num_layers, batch_size, self.hidden_size)).to(device))\n",
    "        elif layer_type == 'gru':\n",
    "            return(autograd.Variable(torch.randn(self.num_layers*2 if self.bidir else self.num_layers, batch_size, self.hidden_size)).to(device))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        if self.layer_type == 'lstm':\n",
    "            self.hidden = self.init_hidden(x.size(0),self.layer_type)\n",
    "            x, (_,_) = self.rnn(x, self.hidden)\n",
    "        elif self.layer_type == 'rnn':\n",
    "            x, _ = self.rnn(x)\n",
    "        else:\n",
    "            self.hidden = self.init_hidden(x.size(0),self.layer_type)\n",
    "            x, _ = self.rnn(x, self.hidden)\n",
    "        x = F.relu(x)        \n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "176385c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_embeddings(model, req_grad=False):\n",
    "    embeddings = model.embed\n",
    "    for c_p in embeddings.parameters():\n",
    "        c_p.requires_grad = req_grad\n",
    "        \n",
    "def training(model, x_column, y_column, criterion, optimizer, scheduler, num_epochs, loaders,\n",
    "                    row, df_result, max_grad_norm=2, num_freeze_iter=1000):\n",
    "    freeze_embeddings(model)\n",
    "    for e in trange(num_epochs, leave=False, desc='Epoch'):\n",
    "        time_start = time.time()\n",
    "        model.train()\n",
    "        num_iter = 0\n",
    "        pbar = tqdm(loaders[\"train\"], leave=False, desc='Train')\n",
    "        y_true_train = []\n",
    "        y_pred_train = []\n",
    "        num_iter = 0\n",
    "        train_loss = 0\n",
    "        num_errors = 0\n",
    "#         for batch_idx, batch in enumerate(loaders[\"train\"]):\n",
    "        for batch in pbar:\n",
    "#             print(batch)\n",
    "            if num_iter > num_freeze_iter and e < 1:\n",
    "                freeze_embeddings(model, True)\n",
    "            optimizer.zero_grad()\n",
    "            input_embeds = batch[x_column].to(device)\n",
    "            labels = batch[y_column].to(device)\n",
    "            prediction = model(input_embeds)\n",
    "            loss = criterion(prediction, labels)\n",
    "            train_loss += loss.item()\n",
    "            for idx, i in enumerate(prediction):\n",
    "                y_pred_train.append(torch.argmax(i).item())\n",
    "            for item in labels:\n",
    "                y_true_train.append(item.item())\n",
    "            loss.backward()\n",
    "            if max_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            num_iter += 1\n",
    "            input_embeds.to(torch.device(\"cpu\"))\n",
    "            labels.to(torch.device(\"cpu\"))\n",
    "            prediction.to(torch.device(\"cpu\"))\n",
    "            loss.to(torch.device(\"cpu\"))         \n",
    "            input_embeds, labels, prediction, loss = None, None, None, None\n",
    "            del input_embeds, labels, prediction, loss\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()    \n",
    "        train_loss = train_loss / (num_iter+1)\n",
    "        train_acc = round(accuracy_score(y_true_train, y_pred_train),3)\n",
    "        train_f1 = round(f1_score(y_true_train, y_pred_train, average='weighted'),3)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            num_iter = 0\n",
    "            y_pred_test = []\n",
    "            y_true_test = []\n",
    "            test_loss = 0\n",
    "            num_errors = 0\n",
    "            pbar_test_eval = tqdm(loaders[\"test\"], leave=False, desc='Test')\n",
    "            for batch in pbar_test_eval:\n",
    "                input_embeds = batch[x_column].to(device)\n",
    "                labels = batch[y_column].to(device)\n",
    "                prediction = model(input_embeds)\n",
    "                loss = criterion(prediction, labels)\n",
    "                test_loss += loss.item()\n",
    "                for idx, i in enumerate(prediction):\n",
    "                    y_pred_test.append(torch.argmax(i).item())\n",
    "                for item in labels:\n",
    "                    y_true_test.append(item.item())\n",
    "                num_iter += 1\n",
    "                input_embeds.to(torch.device(\"cpu\"))\n",
    "                labels.to(torch.device(\"cpu\"))\n",
    "                prediction.to(torch.device(\"cpu\"))\n",
    "                loss.to(torch.device(\"cpu\"))\n",
    "                input_embeds, labels, prediction, loss = None, None, None, None\n",
    "                del input_embeds, labels, prediction, loss\n",
    "               \n",
    "\n",
    "        test_loss = test_loss / (num_iter+1)\n",
    "        test_acc = round(accuracy_score(y_true_test, y_pred_test),3)\n",
    "        test_f1 = round(f1_score(y_true_test, y_pred_test, average='weighted'),3)\n",
    "        time_stop = time.time()\n",
    "        print(f\"epoch: {e}, train loss: {round(train_loss,4)}, test Loss: {round(test_loss,4)}, train acc: {train_acc}, test acc: {test_acc}, time:{round(time_stop-time_start)}\")\n",
    "        ans = [round(train_loss,4), train_acc, train_f1, round(test_loss,4), test_acc, test_f1]\n",
    "        temp_row = row\n",
    "        temp_row.append(e)\n",
    "        for item in ans:\n",
    "            temp_row.append(item)\n",
    "        temp_row.append(round(time_stop-time_start))\n",
    "        df_result.loc[df_result.shape[0]] = temp_row\n",
    "        for i in range(len(ans)+2):\n",
    "            temp_row.pop(-1)\n",
    "     \n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "        y_true_train, y_pred_train, y_pred_test, y_true_test, temp_row = None, None, None, None, None\n",
    "        del y_true_train, y_pred_train, y_pred_test, y_true_test, temp_row\n",
    "        gc.collect()\n",
    "    return df_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7407f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders_kfold(df, n_split, batch_size, collate_fn, get_one, num_iter):\n",
    "    loaders = {}\n",
    "    df = shuffle(df)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    if get_one:\n",
    "        temp_train_df = pd.concat([df_all.loc[df_all.index<df_all.shape[0]/5*num_iter], df_all.loc[df_all.index>=df_all.shape[0]/5*(num_iter+1)]])\n",
    "        temp_test_df = pd.concat([df_all.loc[df_all.index>=df_all.shape[0]/5*num_iter], df_all.loc[df_all.index<df_all.shape[0]/5*(num_iter+1)]])\n",
    "        temp_train_df.reset_index(drop=True, inplace=True)\n",
    "        temp_test_df.reset_index(drop=True, inplace=True)\n",
    "        dataset = DatasetDict()\n",
    "        dataset['train'] = Dataset.from_pandas(temp_train_df)\n",
    "        dataset['test'] = Dataset.from_pandas(temp_test_df)\n",
    "        dataset.set_format(type='torch')\n",
    "        loaders = {'train': torch.utils.data.DataLoader(\n",
    "                      dataset['train'], collate_fn=collate_fn,\n",
    "                      batch_size=batch_size,shuffle=True),\n",
    "                     'test': torch.utils.data.DataLoader(\n",
    "                      dataset['test'], collate_fn=collate_fn,\n",
    "                      batch_size=batch_size,shuffle=False)}\n",
    "        temp_train_df,temp_test_df = None, None; del temp_train_df,temp_test_df; gc.collect()\n",
    "        return loaders\n",
    "        \n",
    "    for i in range(n_split):\n",
    "        temp_train_df = pd.concat([df_all.loc[df_all.index<df_all.shape[0]/5*num_iter], df_all.loc[df_all.index>=df_all.shape[0]/5*(num_iter+1)]])\n",
    "        temp_test_df = pd.concat([df_all.loc[df_all.index>=df_all.shape[0]/5*num_iter], df_all.loc[df_all.index<df_all.shape[0]/5*(num_iter+1)]])\n",
    "        temp_train_df.reset_index(drop=True, inplace=True)\n",
    "        temp_test_df.reset_index(drop=True, inplace=True)\n",
    "        dataset = DatasetDict()\n",
    "        dataset['train'] = Dataset.from_pandas(temp_train_df)\n",
    "        dataset['test'] = Dataset.from_pandas(temp_test_df)\n",
    "        dataset.set_format(type='torch')\n",
    "        loaders[i] = {'train': torch.utils.data.DataLoader(\n",
    "                      dataset['train'], collate_fn=collate_fn,\n",
    "                      batch_size=batch_size,shuffle=True),\n",
    "                     'test': torch.utils.data.DataLoader(\n",
    "                      dataset['test'], collate_fn=collate_fn,\n",
    "                      batch_size=batch_size,shuffle=False)}\n",
    "        temp_train_df,temp_test_df = None, None; del temp_train_df,temp_test_df; gc.collect()\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61207282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/1183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 1.075, test Loss: 1.0563, train acc: 0.383, test acc: 0.405, time:202\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/1183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 1.0311, test Loss: 1.0968, train acc: 0.418, test acc: 0.379, time:197\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/1183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 1.0479, test Loss: 1.0696, train acc: 0.407, test acc: 0.39, time:195\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_type</th>\n",
       "      <th>num_layer</th>\n",
       "      <th>bidir_flg</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>drop_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>num_iter</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lstm</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0750</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.333</td>\n",
       "      <td>1.0563</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.360</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lstm</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0311</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.364</td>\n",
       "      <td>1.0968</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.297</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lstm</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0479</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.363</td>\n",
       "      <td>1.0696</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.337</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  layer_type  num_layer  bidir_flg  hidden_size  drop_rate  batch_size  \\\n",
       "0       lstm          2          0          200        0.1         512   \n",
       "1       lstm          2          0          200        0.1         512   \n",
       "2       lstm          2          0          200        0.1         512   \n",
       "\n",
       "   num_iter  epoch  train_loss  train_acc  train_f1  test_loss  test_acc  \\\n",
       "0         0      0      1.0750      0.383     0.333     1.0563     0.405   \n",
       "1         1      0      1.0311      0.418     0.364     1.0968     0.379   \n",
       "2         2      0      1.0479      0.407     0.363     1.0696     0.390   \n",
       "\n",
       "   test_f1  time  \n",
       "0    0.360   202  \n",
       "1    0.297   197  \n",
       "2    0.337   195  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_embed_layer = True\n",
    "hidden_size = 200\n",
    "drop_rate = 0.1\n",
    "max_length = 128\n",
    "layer_type = 'lstm'\n",
    "bidir_flg = 0\n",
    "lr = 0.001\n",
    "num_epochs=1\n",
    "num_layers=2\n",
    "max_grad_norm = 1.0\n",
    "x_column = 'token'\n",
    "y_column = 'label'\n",
    "batch_size = 512\n",
    "n_split = 3\n",
    "collate_fn = Collator(max_length,x_column=x_column, y_column=y_column)\n",
    "columns=['layer_type','num_layer','bidir_flg','hidden_size','drop_rate','batch_size','num_iter',\n",
    "         'epoch','train_loss','train_acc','train_f1',\n",
    "         'test_loss','test_acc','test_f1','time']\n",
    "df_result = pd.DataFrame([], columns=columns)\n",
    "\n",
    "for num_iter in range(n_split):\n",
    "    row = [layer_type, num_layers, bidir_flg, hidden_size, drop_rate, batch_size, num_iter]\n",
    "    model = GRUModel(use_embed_layer, word2vec.vector_size, hidden_size, \n",
    "                 drop_rate,max_length,layer_type, bidir_flg,num_layers).to(device)\n",
    "    model.apply(init_weights)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=3, gamma=0.9)\n",
    "    loaders = create_dataloaders_kfold(df=df_all, n_split=n_split, batch_size=batch_size, collate_fn=collate_fn, get_one=True, num_iter=num_iter)\n",
    "    df_result = training(model, x_column, y_column, criterion, optimizer, scheduler, num_epochs, loaders, row, df_result, max_grad_norm) \n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ab6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d54bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb39d8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5c353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd63942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "2c5b89e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a9b66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402cc5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f5a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7802d22f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
